
- [x] define intrinsic/extrinsic evaluation in [[Word2Vec]]
- [x] complete notes on [[t-SNE]]
- [x] [[Beam Search]]
- [ ] complete [[Calculating matrix sizes for self attention in encoder decoder networks]] 
- [x] [[Teacher Student Forcing]]
- [x] knowledge distillation


---

Quiz 4 Opens up **October 30th at 8am ET,** and covers Structured Representations (Lesson 11), Language Models (Lesson 12), Embeddings (Lesson 13), Neural Attention Models (Lesson 14), Neural Machine Translation (Lesson 15), and Advanced Topics (Lesson 16).

Conceptual Questions:

- RNNs and LSTMs, how their update rules differ, and what problems they each have or solve
    
- Conditional language models and how to train them (teacher/student forcing), language metrics (how to calculate them), how knowledge distillation works
    
- Word embeddings and details of word2vec/skip-gram model (form of probability equation, what is conditioned on what, intrinsic/extrinsic evaluation)
    
- What t-SNE is and how it conceptually works
    
- The encoder-decoder model and how it's used by recurrent neural networks for seq2seq prediction problems.
    
- Beam search.
    
- Neural attention (different types, how it's computed); "Attention is all you Need"
    
- Transformers (architecture details, positional encoding, computational complexity)
    
- Byte pair encoding.
    

Computation questions:

- Please be familiar with how to calculate the sizes of the different self-attention matrices in the encoder, decoder, and between the encoder and decoder (often called cross-attention) based on "Attention is all you Need."

![[M3L11 Introduction to Structured Representations - Slides.pdf]]

![[M3L12 Language Models (Maillard) - Slides v2.pdf]]![[M3L13 Embeddings (Wu) - Slides v3.pdf]]![[M3L14 Neural Attention Models (Szlam) - Slides v3.pdf]]![[M3L15 Neural Machine Translation (Cross) - Slides v2.pdf]]![[M3L16 Translation at Facebook (Guzman) - Slides v2.pdf]]