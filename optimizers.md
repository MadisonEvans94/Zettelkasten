#seed 
upstream:

---

**video links**: 

---

# Brain Dump: 

- SGD 
- issues in optimization 
	- noisy gradient estimates 
	- saddle points 
- Momentum 
- second order approximation of loss 
- SGD with momentum requires tuning 
- Optimizers 
	- Adagrad 
	- RMSProp 
	- Adam 


--- 




>saddle points 

![[Screen Shot 2023-09-17 at 10.44.59 AM.png]]


>momentum: when beta is 0 then that's just normal sgd 

![[Screen Shot 2023-09-17 at 10.48.51 AM.png]]

![[Screen Shot 2023-09-17 at 10.49.12 AM.png]]

Hessian and Loss Curvature 
![[Screen Shot 2023-09-17 at 10.53.36 AM.png]]


Adagrad 
![[Screen Shot 2023-09-17 at 10.57.04 AM.png]]

RMSProp
![[Screen Shot 2023-09-17 at 11.00.22 AM.png]]

Adam
![[Screen Shot 2023-09-17 at 11.00.48 AM.png]]